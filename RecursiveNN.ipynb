{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gensim\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "pp=pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "#param\n",
    "nlpserver=\"http://localhost:9000\"\n",
    "vector_size=100\n",
    "corpus_file=\"enwiki-20150112-400-r100-10576.txt\"\n",
    "word2vec_model_file=\"word2vec.mod\"\n",
    "\n",
    "debug=True #when debug is true, you need to use vector_size=5 word2vec model\n",
    "\n",
    "lr=0.01\n",
    "lambda_2=0.2\n",
    "#batch_size="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp=StanfordCoreNLP(nlpserver)\n",
    "properties={'annotators':'parse','outputFormat':'json'}\n",
    "\n",
    "#test CoreNLP server\n",
    "print nlp.annotate(\"This is a pen which I bought\",properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence=\"This is a pen. Sky is bleu\"\n",
    "pp.pprint(nlp.annotate(sentence,properties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load(or create) word2vec model \n",
    "def load_word2vec_model(model_file):\n",
    "    if os.path.exists(model_file):\n",
    "        model= wv.Word2Vec.load_word2vec_format(model_file,binary=False,unicode_errors='ignore')\n",
    "    else:\n",
    "        data =word2vec.Text8Corpus(corpus_file)\n",
    "        model=word2vec.Word2Vec(data,size=vector_size)\n",
    "        #model.save(model_file)\n",
    "    \n",
    "    return model\n",
    "\n",
    "#test word2vec\n",
    "model=load_word2vec_model(word2vec_model_file)\n",
    "out=model.most_similar(positive=[\"apple\"])\n",
    "for x in out:\n",
    "    print x[0],x[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CoreNLP \"parse\" annotator JSON format\n",
    "...\n",
    "u'basic-dependencies':\n",
    "    {  u'dep': u'ROOT', \n",
    "    u'dependent': 4, #term ID\n",
    "    u'dependentGloss': u'pen', #term text\n",
    "    u'governor': 0, #\n",
    "    u'governorGloss': u'ROOT'},\n",
    "...\n",
    "\n",
    "\"\"\"\n",
    "class TermNode:\n",
    "    def __init__(self,param):\n",
    "        self.term=param[\"dependentGloss\"]\n",
    "        self.param=param\n",
    "        self.childs=[]\n",
    "    \n",
    "    def add_child(self,child):\n",
    "        self.childs.append(child)\n",
    "    \n",
    "    def find_id(self,node_id):\n",
    "        if self.param[\"dependent\"]==node_id:\n",
    "            return self\n",
    "        else:\n",
    "            for child in self.childs:\n",
    "                result=child.find_id(node_id)\n",
    "                if result!=None:\n",
    "                    return result\n",
    "            return None\n",
    "    \n",
    "    def get_training_data(self,session,rnn_op,word2vec_model):\n",
    "        training_data=[]\n",
    "        #process child node\n",
    "        for child in self.childs:\n",
    "            child_data=child.get_training_data(session,rnn_op,word2vec_model)\n",
    "            if len(child_data)>0: #when leaf node, child_data will be None\n",
    "                training_data.extend(child_data)\n",
    "        \n",
    "        #calculate RNN output on this node\n",
    "        #calculate RNN output and use it for next input data\n",
    "        try:\n",
    "            rnn_result=word2vec_model[self.term]\n",
    "        except KeyError:\n",
    "            vector_size=len(word2vec_model[\"apple\"])\n",
    "            rnn_result=[0 for i in range(vector_size)]\n",
    "        vector_size=len(rnn_result)\n",
    "        for child in self.childs:\n",
    "            #concatinate former iteration RNN result and next child node vector,and make it training data\n",
    "            concatinated=vector_size*2*[0]\n",
    "            for i in range(len(rnn_result)):\n",
    "                concatinated[i]=rnn_result[i]\n",
    "            for i in range(len(child.rnn_result)):\n",
    "                concatinated[i+vector_size]=child.rnn_result[i]\n",
    "            training_data.append(concatinated)\n",
    "            \n",
    "            #calculate next RNN output\n",
    "            x_input=np.array([concatinated])\n",
    "            #try:\n",
    "            rnn_result=session.run(rnn_op,feed_dict={x:x_input})\n",
    "#             except ValueError:\n",
    "#                 print x_input\n",
    "            rnn_result=[elem for elem in rnn_result[0]]\n",
    "            \n",
    "        #memorize final rnn output as feature for this node\n",
    "        self.rnn_result=rnn_result\n",
    "        \n",
    "        return training_data\n",
    "\n",
    "def check_dependency_format(basic_dependency):\n",
    "    keys=[\"dependent\",\"governor\",\"dependentGloss\"]\n",
    "    for key in keys:\n",
    "        if not key in basic_dependency.keys():\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "#arg : basic-dependencies result of CoreNLP for a sentence\n",
    "#return : term tree structure  \n",
    "def get_term_tree(basic_dependencies):\n",
    "    #before processing checking result format\n",
    "    for basic_dependency in basic_dependencies:\n",
    "        if not check_dependency_format(basic_dependency):\n",
    "            return None\n",
    "        \n",
    "    root_node=TermNode(basic_dependencies[0])\n",
    "    node_dict={basic_dependencies[0][\"dependent\"]:root_node}\n",
    "    \n",
    "    #construct all node\n",
    "    for i in range(1,len(basic_dependencies)):\n",
    "        node_dict[basic_dependencies[i][\"dependent\"]]=TermNode(basic_dependencies[i])\n",
    "    \n",
    "    #make node into tree \n",
    "    for i in range(1,len(basic_dependencies)):\n",
    "        parent_node=node_dict[basic_dependencies[i][\"governor\"]]\n",
    "        parent_node.add_child(node_dict[basic_dependencies[i][\"dependent\"]])\n",
    " \n",
    "    return root_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def build_auto_encoder(vector_size,lr,lambda_2):\n",
    "x=tf.placeholder(tf.float32,[None,vector_size*2])\n",
    "hidden_size=vector_size\n",
    "#input layer \n",
    "# dict_size x 2\n",
    "W_in=tf.Variable(tf.zeros([vector_size*2,hidden_size]))\n",
    "b_in=tf.Variable(tf.zeros([hidden_size]))\n",
    "\n",
    "hidden_y=tf.nn.sigmoid(tf.matmul(x,W_in)+b_in)\n",
    "\n",
    "#output layer\n",
    "# 2x2\n",
    "W_out=tf.Variable(tf.zeros([hidden_size,vector_size*2]))\n",
    "b_out=tf.Variable(tf.zeros([vector_size*2]))\n",
    "\n",
    "#predictions\n",
    "y=tf.nn.sigmoid(tf.matmul(hidden_y,W_out)+b_out)\n",
    "\n",
    "#true label\n",
    "y_=tf.placeholder(tf.float32,[None,vector_size*2])\n",
    "\n",
    "#learning process\n",
    "cost = tf.reduce_mean(tf.reduce_sum(tf.pow(y-y_,2)))\n",
    "L2_sqr=tf.nn.l2_loss(W_in)+tf.nn.l2_loss(W_out)\n",
    "loss=cost+lambda_2*L2_sqr\n",
    "train_step = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "#return train_step,loss,hidden_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_mini_batch(session,hidden_y,word2vec_model,corpus_itr,batch_sentence=2):\n",
    "    sentence=\"\"\n",
    "    for i in range(batch_sentence):\n",
    "        sentence+=corpus_itr.next().rstrip()+\" \"\n",
    "    x_input=[]\n",
    "    try:\n",
    "        annotated=nlp.annotate(sentence,properties)\n",
    "    \n",
    "    #error handling of core nlp\n",
    "    except UnicodeDecodeError:\n",
    "        return None\n",
    "    if not isinstance(annotated,dict):\n",
    "        return None\n",
    "    \n",
    "    for sentence in annotated[\"sentences\"]:\n",
    "        root_node=get_term_tree(sentence[\"basic-dependencies\"])\n",
    "        if root_node==None:\n",
    "            return None\n",
    "        x_input.extend(root_node.get_training_data(session,hidden_y,word2vec_model))\n",
    "    return np.array(x_input)\n",
    "\n",
    "\n",
    "#train_step,loss,hiddeny=build_auto_encoder(vector_size,lr,lambda_2)\n",
    "init = tf.initialize_all_variables()\n",
    "sess=tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "#train auto encoder\n",
    "#here, input x and output y_ are same \n",
    "skipped=0\n",
    "with iter(open(corpus_file,\"r\")) as corpus_itr:\n",
    "    for i in range(10000):\n",
    "        x_input=get_mini_batch(sess,hidden_y,model,corpus_itr)\n",
    "        if x_input is None or len(x_input)==0:\n",
    "            skipped+=1\n",
    "            continue\n",
    "            \n",
    "        result=sess.run([train_step,cost,loss],feed_dict={x:x_input,y_:x_input})\n",
    "        if i%100==0:\n",
    "            print result,skipped\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test\n",
    "if debut==True:\n",
    "    x_input=get_mini_batch(sess,hidden_y,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print os.path.exists(corpus_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(corpus_file,\"r\") as corpus:\n",
    "    print len(corpus.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dic={}\n",
    "print isinstance(dic,dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
