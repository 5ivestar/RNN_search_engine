{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from gensim.models import word2vec\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "\n",
    "corpus_file=\"data/enwiki-20150112-400-r10-105752.txt\"\n",
    "term_file=\"data/4000-most-common-english-words-csv.csv\"\n",
    "word2vec_model_file=\"word2vec_v2.mod\"\n",
    "vector_size=200\n",
    "nlpserver=\"http://localhost:9000\"\n",
    "model_directory=\"model/\"\n",
    "epoch=200\n",
    "lr=0.1\n",
    "lambda_2=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('apple.n.01'), Synset('apple.n.02')]\n",
      "a unit of language that native speakers can identify\n"
     ]
    }
   ],
   "source": [
    "print wn.synsets(\"apple\")\n",
    "print wn.synsets(\"word\")[0].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple, 0.877130806446\n",
      "coconut 0.801503241062\n",
      "edible 0.800157427788\n",
      "almond 0.799016475677\n",
      "olive 0.797267079353\n",
      "bean 0.793999910355\n",
      "butter 0.790080070496\n",
      "onion 0.785118877888\n",
      "mango 0.783805608749\n",
      "fruit, 0.781246781349\n"
     ]
    }
   ],
   "source": [
    "#word2vec learning\n",
    "#load(or create) word2vec model \n",
    "def load_word2vec_model(model_file):\n",
    "    if os.path.exists(model_file):\n",
    "        model= word2vec.Word2Vec.load(model_file)\n",
    "    else:\n",
    "        data =word2vec.Text8Corpus(corpus_file)\n",
    "        model=word2vec.Word2Vec(data,size=vector_size)\n",
    "        model.save(model_file)\n",
    "    \n",
    "    return model\n",
    "\n",
    "#test\n",
    "model=load_word2vec_model(word2vec_model_file)\n",
    "out=model.most_similar(positive=[\"apple\"])\n",
    "for x in out:\n",
    "    print x[0],x[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'sentences': [{u'tokens': [{u'index': 1, u'word': u'fruit', u'after': u' ', u'pos': u'VBN', u'characterOffsetEnd': 5, u'characterOffsetBegin': 0, u'originalText': u'fruit', u'before': u''}, {u'index': 2, u'word': u'with', u'after': u' ', u'pos': u'IN', u'characterOffsetEnd': 10, u'characterOffsetBegin': 6, u'originalText': u'with', u'before': u' '}, {u'index': 3, u'word': u'red', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 14, u'characterOffsetBegin': 11, u'originalText': u'red', u'before': u' '}, {u'index': 4, u'word': u'or', u'after': u' ', u'pos': u'CC', u'characterOffsetEnd': 17, u'characterOffsetBegin': 15, u'originalText': u'or', u'before': u' '}, {u'index': 5, u'word': u'yellow', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 24, u'characterOffsetBegin': 18, u'originalText': u'yellow', u'before': u' '}, {u'index': 6, u'word': u'or', u'after': u' ', u'pos': u'CC', u'characterOffsetEnd': 27, u'characterOffsetBegin': 25, u'originalText': u'or', u'before': u' '}, {u'index': 7, u'word': u'green', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 33, u'characterOffsetBegin': 28, u'originalText': u'green', u'before': u' '}, {u'index': 8, u'word': u'skin', u'after': u' ', u'pos': u'NN', u'characterOffsetEnd': 38, u'characterOffsetBegin': 34, u'originalText': u'skin', u'before': u' '}, {u'index': 9, u'word': u'and', u'after': u' ', u'pos': u'CC', u'characterOffsetEnd': 42, u'characterOffsetBegin': 39, u'originalText': u'and', u'before': u' '}, {u'index': 10, u'word': u'sweet', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 48, u'characterOffsetBegin': 43, u'originalText': u'sweet', u'before': u' '}, {u'index': 11, u'word': u'to', u'after': u' ', u'pos': u'TO', u'characterOffsetEnd': 51, u'characterOffsetBegin': 49, u'originalText': u'to', u'before': u' '}, {u'index': 12, u'word': u'tart', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 56, u'characterOffsetBegin': 52, u'originalText': u'tart', u'before': u' '}, {u'index': 13, u'word': u'crisp', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 62, u'characterOffsetBegin': 57, u'originalText': u'crisp', u'before': u' '}, {u'index': 14, u'word': u'whitish', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 70, u'characterOffsetBegin': 63, u'originalText': u'whitish', u'before': u' '}, {u'index': 15, u'word': u'flesh', u'after': u'', u'pos': u'NN', u'characterOffsetEnd': 76, u'characterOffsetBegin': 71, u'originalText': u'flesh', u'before': u' '}], u'index': 0, u'basic-dependencies': [{u'dep': u'ROOT', u'dependent': 1, u'governorGloss': u'ROOT', u'governor': 0, u'dependentGloss': u'fruit'}, {u'dep': u'case', u'dependent': 2, u'governorGloss': u'skin', u'governor': 8, u'dependentGloss': u'with'}, {u'dep': u'dep', u'dependent': 3, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'red'}, {u'dep': u'cc', u'dependent': 4, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'or'}, {u'dep': u'amod', u'dependent': 5, u'governorGloss': u'skin', u'governor': 8, u'dependentGloss': u'yellow'}, {u'dep': u'cc', u'dependent': 6, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'or'}, {u'dep': u'conj', u'dependent': 7, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'green'}, {u'dep': u'nmod', u'dependent': 8, u'governorGloss': u'fruit', u'governor': 1, u'dependentGloss': u'skin'}, {u'dep': u'cc', u'dependent': 9, u'governorGloss': u'fruit', u'governor': 1, u'dependentGloss': u'and'}, {u'dep': u'conj', u'dependent': 10, u'governorGloss': u'fruit', u'governor': 1, u'dependentGloss': u'sweet'}, {u'dep': u'case', u'dependent': 11, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'to'}, {u'dep': u'amod', u'dependent': 12, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'tart'}, {u'dep': u'amod', u'dependent': 13, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'crisp'}, {u'dep': u'amod', u'dependent': 14, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'whitish'}, {u'dep': u'nmod', u'dependent': 15, u'governorGloss': u'sweet', u'governor': 10, u'dependentGloss': u'flesh'}], u'parse': u'(ROOT\\r\\n  (UCP\\r\\n    (VP (VBN fruit)\\r\\n      (PP (IN with)\\r\\n        (NP\\r\\n          (ADJP (JJ red)\\r\\n            (CC or)\\r\\n            (JJ yellow)\\r\\n            (CC or)\\r\\n            (JJ green))\\r\\n          (NN skin))))\\r\\n    (CC and)\\r\\n    (ADJP (JJ sweet)\\r\\n      (PP (TO to)\\r\\n        (NP (JJ tart) (JJ crisp) (JJ whitish) (NN flesh))))))', u'collapsed-dependencies': [{u'dep': u'ROOT', u'dependent': 1, u'governorGloss': u'ROOT', u'governor': 0, u'dependentGloss': u'fruit'}, {u'dep': u'case', u'dependent': 2, u'governorGloss': u'skin', u'governor': 8, u'dependentGloss': u'with'}, {u'dep': u'dep', u'dependent': 3, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'red'}, {u'dep': u'cc', u'dependent': 4, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'or'}, {u'dep': u'amod', u'dependent': 5, u'governorGloss': u'skin', u'governor': 8, u'dependentGloss': u'yellow'}, {u'dep': u'cc', u'dependent': 6, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'or'}, {u'dep': u'conj:or', u'dependent': 7, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'green'}, {u'dep': u'nmod:with', u'dependent': 8, u'governorGloss': u'fruit', u'governor': 1, u'dependentGloss': u'skin'}, {u'dep': u'cc', u'dependent': 9, u'governorGloss': u'fruit', u'governor': 1, u'dependentGloss': u'and'}, {u'dep': u'conj:and', u'dependent': 10, u'governorGloss': u'fruit', u'governor': 1, u'dependentGloss': u'sweet'}, {u'dep': u'case', u'dependent': 11, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'to'}, {u'dep': u'amod', u'dependent': 12, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'tart'}, {u'dep': u'amod', u'dependent': 13, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'crisp'}, {u'dep': u'amod', u'dependent': 14, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'whitish'}, {u'dep': u'nmod:to', u'dependent': 15, u'governorGloss': u'sweet', u'governor': 10, u'dependentGloss': u'flesh'}], u'collapsed-ccprocessed-dependencies': [{u'dep': u'ROOT', u'dependent': 1, u'governorGloss': u'ROOT', u'governor': 0, u'dependentGloss': u'fruit'}, {u'dep': u'case', u'dependent': 2, u'governorGloss': u'skin', u'governor': 8, u'dependentGloss': u'with'}, {u'dep': u'dep', u'dependent': 3, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'red'}, {u'dep': u'cc', u'dependent': 4, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'or'}, {u'dep': u'amod', u'dependent': 5, u'governorGloss': u'skin', u'governor': 8, u'dependentGloss': u'yellow'}, {u'dep': u'cc', u'dependent': 6, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'or'}, {u'dep': u'conj:or', u'dependent': 7, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'green'}, {u'dep': u'amod', u'dependent': 7, u'governorGloss': u'skin', u'governor': 8, u'dependentGloss': u'green'}, {u'dep': u'nmod:with', u'dependent': 8, u'governorGloss': u'fruit', u'governor': 1, u'dependentGloss': u'skin'}, {u'dep': u'cc', u'dependent': 9, u'governorGloss': u'fruit', u'governor': 1, u'dependentGloss': u'and'}, {u'dep': u'conj:and', u'dependent': 10, u'governorGloss': u'fruit', u'governor': 1, u'dependentGloss': u'sweet'}, {u'dep': u'case', u'dependent': 11, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'to'}, {u'dep': u'amod', u'dependent': 12, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'tart'}, {u'dep': u'amod', u'dependent': 13, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'crisp'}, {u'dep': u'amod', u'dependent': 14, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'whitish'}, {u'dep': u'nmod:to', u'dependent': 15, u'governorGloss': u'sweet', u'governor': 10, u'dependentGloss': u'flesh'}]}]}\n"
     ]
    }
   ],
   "source": [
    "nlp=StanfordCoreNLP(nlpserver)\n",
    "properties={'annotators':'parse','outputFormat':'json'}\n",
    "\n",
    "#test CoreNLP server\n",
    "print nlp.annotate(\"fruit with red or yellow or green skin and sweet to tart crisp whitish flesh\",properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CoreNLP \"parse\" annotator JSON format\n",
    "...\n",
    "u'basic-dependencies':\n",
    "    {  u'dep': u'ROOT', \n",
    "    u'dependent': 4, #term ID\n",
    "    u'dependentGloss': u'pen', #term text\n",
    "    u'governor': 0, #\n",
    "    u'governorGloss': u'ROOT'},\n",
    "...\n",
    "\n",
    "\"\"\"\n",
    "class TermNode:\n",
    "    def __init__(self,param):\n",
    "        self.term=param[\"dependentGloss\"]\n",
    "        self.param=param\n",
    "        self.childs=[]\n",
    "    \n",
    "    def add_child(self,child):\n",
    "        self.childs.append(child)\n",
    "    \n",
    "    def find_id(self,node_id):\n",
    "        if self.param[\"dependent\"]==node_id:\n",
    "            return self\n",
    "        else:\n",
    "            for child in self.childs:\n",
    "                result=child.find_id(node_id)\n",
    "                if result!=None:\n",
    "                    return result\n",
    "            return None\n",
    "    \n",
    "    def get_training_data(self,word2vec_model,keep_prob):\n",
    "        #process child node\n",
    "        for child in self.childs:\n",
    "            child.get_training_data(word2vec_model,keep_prob)\n",
    "        \n",
    "        #calculate RNN output on this node\n",
    "        #calculate RNN output and use it for next input data\n",
    "        try:\n",
    "            rnn_result=word2vec_model[self.term]\n",
    "        except KeyError:\n",
    "            vector_size=len(word2vec_model[\"apple\"])\n",
    "            rnn_result=[0.0 for i in range(vector_size)]\n",
    "            \n",
    "        rnn_result=tf.convert_to_tensor([[value for value in rnn_result]])\n",
    "        no_droped=rnn_result\n",
    "        for child in self.childs:\n",
    "            #concatinate former iteration RNN result and next child node vector,and make it input data\n",
    "            x=tf.concat(1,[child.rnn_result,rnn_result])\n",
    "            rnn_result,no_droped=process_NN(x,keep_prob)\n",
    "            \n",
    "        #memorize final rnn output as feature for this node\n",
    "        self.rnn_result=rnn_result\n",
    "        self.no_droped=no_droped\n",
    "        \n",
    "\n",
    "def check_dependency_format(basic_dependency):\n",
    "    keys=[\"dependent\",\"governor\",\"dependentGloss\"]\n",
    "    for key in keys:\n",
    "        if not key in basic_dependency.keys():\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "#arg : basic-dependencies result of CoreNLP for a sentence\n",
    "#return : term tree structure  \n",
    "def get_term_tree(basic_dependencies):\n",
    "    #before processing checking result format\n",
    "    for basic_dependency in basic_dependencies:\n",
    "        if not check_dependency_format(basic_dependency):\n",
    "            return None\n",
    "        \n",
    "    root_node=TermNode(basic_dependencies[0])\n",
    "    node_dict={basic_dependencies[0][\"dependent\"]:root_node}\n",
    "    \n",
    "    #construct all node\n",
    "    for i in range(1,len(basic_dependencies)):\n",
    "        node_dict[basic_dependencies[i][\"dependent\"]]=TermNode(basic_dependencies[i])\n",
    "    \n",
    "    #make node into tree \n",
    "    for i in range(1,len(basic_dependencies)):\n",
    "        parent_node=node_dict[basic_dependencies[i][\"governor\"]]\n",
    "        parent_node.add_child(node_dict[basic_dependencies[i][\"dependent\"]])\n",
    " \n",
    "    return root_node\n",
    "\n",
    "# x: vector size*2\n",
    "#output : vector size\n",
    "def process_NN(x,keep_prob):\n",
    "    with tf.variable_scope(\"composition\",reuse=True):\n",
    "        w=tf.get_variable(\"weight\")\n",
    "        b=tf.get_variable(\"biase\")\n",
    "\n",
    "    y=tf.nn.sigmoid(tf.matmul(x,w)+b)\n",
    "    y_drop=tf.nn.dropout(y,keep_prob)\n",
    "    \n",
    "    return y_drop,y\n",
    "\n",
    "def printt(x):\n",
    "    print x.get_shape().dims,x.get_shape().ndims\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def build_auto_encoder(vector_size,lr,lambda_2):\n",
    "\n",
    "keep_prob=tf.placeholder(\"float\")\n",
    "\n",
    "def define_NN(input_size,layer_defs):\n",
    "    L2_sqr=0\n",
    "    with tf.variable_scope(\"composition\"):\n",
    "            w_h=tf.get_variable(\"weight\",[vector_size*2,vector_size]\n",
    "                                ,initializer=tf.random_normal_initializer())\n",
    "            b_h=tf.get_variable(\"biase\",vector_size\n",
    "                                ,initializer=tf.random_normal_initializer())\n",
    "   \n",
    "    L2_sqr=L2_sqr+tf.nn.l2_loss(w_h)\n",
    "    tf.scalar_summary(\"L2_loss\",L2_sqr)\n",
    "    \n",
    "    return L2_sqr\n",
    "\n",
    "def variable_summarizer(var,name):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.scalar_summary('mean/' + name, mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.scalar_summary('stddev/' + name, stddev)\n",
    "        tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "        tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "        tf.histogram_summary(name, var)\n",
    "    \n",
    "\n",
    "L2_sqr=define_NN(vector_size*2,[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load frequent term from term_file\n",
    "with open(term_file,\"r\") as f:\n",
    "    terms=[term.rstrip() for term in f.readlines() ]\n",
    "terms=terms[500:3500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch session\n",
      "0 503.911 202.86 301.05\n",
      "10 478.868 199.375 279.493\n",
      "20 585.906 195.747 390.16\n",
      "30 566.417 192.252 374.165\n",
      "40 345.545 188.978 156.567\n",
      "50 515.113 185.95 329.164\n",
      "60 438.011 182.886 255.126\n",
      "70 522.504 179.763 342.741\n",
      "80 486.12 176.887 309.233\n",
      "90 409.369 174.732 234.637\n",
      "100 514.892 172.149 342.742\n",
      "110 641.646 169.314 472.332\n",
      "120 598.35 166.735 431.615\n",
      "130 457.959 163.69 294.269\n",
      "140 311.122 160.894 150.228\n",
      "150 486.195 158.158 328.037\n",
      "160 544.34 155.516 388.825\n",
      "170 412.405 152.781 259.624\n",
      "180 459.535 149.981 309.554\n",
      "190 457.325 147.352 309.972\n",
      "200 epoch session\n",
      "0 362.439 145.014 217.426\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f273c74fdd93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mroot_node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/takashi/.pyenv/versions/2.7.12/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 710\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    711\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/takashi/.pyenv/versions/2.7.12/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 908\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/takashi/.pyenv/versions/2.7.12/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 958\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    959\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/takashi/.pyenv/versions/2.7.12/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    963\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/takashi/.pyenv/versions/2.7.12/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    945\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    946\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_model_path():\n",
    "    return model_directory+\"lr_%f-lambda2_%f-vector_size_%d\" %(lr,lambda_2,vector_size)\n",
    "\n",
    "new_model=False\n",
    "\n",
    "\n",
    "sess=tf.Session()\n",
    "if new_model:\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "else:\n",
    "    saver=tf.train.Saver()\n",
    "    saver.restore(sess,get_model_path())\n",
    "\n",
    "skipped=0\n",
    "\n",
    "for i in range(epoch):\n",
    "    term=terms[random.randrange(len(terms))]\n",
    "    try:\n",
    "        termVec=model[term]\n",
    "    except KeyError:\n",
    "        skipped+=1\n",
    "        continue\n",
    "\n",
    "    synsets=wn.synsets(term)\n",
    "    if len(synsets)==0:\n",
    "        skipped+=1\n",
    "        continue\n",
    "\n",
    "    #use first synset definition \n",
    "    definition=str(synsets[0].definition())\n",
    "    try:\n",
    "        annotated=nlp.annotate(definition,properties)\n",
    "\n",
    "    #error handling of core nlp\n",
    "    except UnicodeDecodeError:\n",
    "        skipped+=1\n",
    "        continue\n",
    "    if not isinstance(annotated,dict):\n",
    "        skipped+=1\n",
    "        continue\n",
    "    #use only first sentence\n",
    "    sentence=annotated[\"sentences\"][0]\n",
    "    root_node=get_term_tree(sentence[\"basic-dependencies\"])\n",
    "    if root_node==None:\n",
    "        skipped+=1\n",
    "        continue\n",
    "\n",
    "    #making feed back phase for RNN\n",
    "    root_node.get_training_data(model,keep_prob)\n",
    "\n",
    "    rnn_result=root_node.no_droped\n",
    "    true_label=model[term]\n",
    "\n",
    "    cost=tf.reduce_mean(tf.reduce_sum(tf.pow(rnn_result-true_label,2)))\n",
    "    loss=cost+lambda_2*L2_sqr\n",
    "    train_step = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "\n",
    "\n",
    "    #training\n",
    "    if i%10==0:\n",
    "        result=sess.run([train_step,loss,cost],feed_dict={keep_prob:0.5})\n",
    "        print i,result[1],result[1]-result[2],result[2]\n",
    "    else:\n",
    "        sess.run([train_step],feed_dict={keep_prob:0.5})\n",
    "\n",
    "    del root_node\n",
    "    gc.collect()\n",
    "\n",
    "saver=tf.train.Saver()\n",
    "saver.save(sess,get_model_path())\n",
    "sess.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
