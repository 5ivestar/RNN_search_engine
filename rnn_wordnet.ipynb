{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from gensim.models import word2vec\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "\n",
    "corpus_file=\"data/enwiki-20150112-400-r10-105752.txt\"\n",
    "term_file=\"data/4000-most-common-english-words-csv.csv\"\n",
    "word2vec_model_file=\"word2vec_v2.mod\"\n",
    "vector_size=200\n",
    "nlpserver=\"http://localhost:9000\"\n",
    "epoch=10000\n",
    "lr=0.1\n",
    "lambda_2=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('apple.n.01'), Synset('apple.n.02')]\n",
      "a unit of language that native speakers can identify\n"
     ]
    }
   ],
   "source": [
    "print wn.synsets(\"apple\")\n",
    "print wn.synsets(\"word\")[0].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple, 0.877130806446\n",
      "coconut 0.801503241062\n",
      "edible 0.800157427788\n",
      "almond 0.799016475677\n",
      "olive 0.797267079353\n",
      "bean 0.793999910355\n",
      "butter 0.790080070496\n",
      "onion 0.785118877888\n",
      "mango 0.783805608749\n",
      "fruit, 0.781246781349\n"
     ]
    }
   ],
   "source": [
    "#word2vec learning\n",
    "#load(or create) word2vec model \n",
    "def load_word2vec_model(model_file):\n",
    "    if os.path.exists(model_file):\n",
    "        model= word2vec.Word2Vec.load(model_file)\n",
    "    else:\n",
    "        data =word2vec.Text8Corpus(corpus_file)\n",
    "        model=word2vec.Word2Vec(data,size=vector_size)\n",
    "        model.save(model_file)\n",
    "    \n",
    "    return model\n",
    "\n",
    "#test\n",
    "model=load_word2vec_model(word2vec_model_file)\n",
    "out=model.most_similar(positive=[\"apple\"])\n",
    "for x in out:\n",
    "    print x[0],x[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'sentences': [{u'tokens': [{u'index': 1, u'word': u'fruit', u'after': u' ', u'pos': u'VBN', u'characterOffsetEnd': 5, u'characterOffsetBegin': 0, u'originalText': u'fruit', u'before': u''}, {u'index': 2, u'word': u'with', u'after': u' ', u'pos': u'IN', u'characterOffsetEnd': 10, u'characterOffsetBegin': 6, u'originalText': u'with', u'before': u' '}, {u'index': 3, u'word': u'red', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 14, u'characterOffsetBegin': 11, u'originalText': u'red', u'before': u' '}, {u'index': 4, u'word': u'or', u'after': u' ', u'pos': u'CC', u'characterOffsetEnd': 17, u'characterOffsetBegin': 15, u'originalText': u'or', u'before': u' '}, {u'index': 5, u'word': u'yellow', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 24, u'characterOffsetBegin': 18, u'originalText': u'yellow', u'before': u' '}, {u'index': 6, u'word': u'or', u'after': u' ', u'pos': u'CC', u'characterOffsetEnd': 27, u'characterOffsetBegin': 25, u'originalText': u'or', u'before': u' '}, {u'index': 7, u'word': u'green', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 33, u'characterOffsetBegin': 28, u'originalText': u'green', u'before': u' '}, {u'index': 8, u'word': u'skin', u'after': u' ', u'pos': u'NN', u'characterOffsetEnd': 38, u'characterOffsetBegin': 34, u'originalText': u'skin', u'before': u' '}, {u'index': 9, u'word': u'and', u'after': u' ', u'pos': u'CC', u'characterOffsetEnd': 42, u'characterOffsetBegin': 39, u'originalText': u'and', u'before': u' '}, {u'index': 10, u'word': u'sweet', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 48, u'characterOffsetBegin': 43, u'originalText': u'sweet', u'before': u' '}, {u'index': 11, u'word': u'to', u'after': u' ', u'pos': u'TO', u'characterOffsetEnd': 51, u'characterOffsetBegin': 49, u'originalText': u'to', u'before': u' '}, {u'index': 12, u'word': u'tart', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 56, u'characterOffsetBegin': 52, u'originalText': u'tart', u'before': u' '}, {u'index': 13, u'word': u'crisp', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 62, u'characterOffsetBegin': 57, u'originalText': u'crisp', u'before': u' '}, {u'index': 14, u'word': u'whitish', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 70, u'characterOffsetBegin': 63, u'originalText': u'whitish', u'before': u' '}, {u'index': 15, u'word': u'flesh', u'after': u'', u'pos': u'NN', u'characterOffsetEnd': 76, u'characterOffsetBegin': 71, u'originalText': u'flesh', u'before': u' '}], u'index': 0, u'basic-dependencies': [{u'dep': u'ROOT', u'dependent': 1, u'governorGloss': u'ROOT', u'governor': 0, u'dependentGloss': u'fruit'}, {u'dep': u'case', u'dependent': 2, u'governorGloss': u'skin', u'governor': 8, u'dependentGloss': u'with'}, {u'dep': u'dep', u'dependent': 3, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'red'}, {u'dep': u'cc', u'dependent': 4, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'or'}, {u'dep': u'amod', u'dependent': 5, u'governorGloss': u'skin', u'governor': 8, u'dependentGloss': u'yellow'}, {u'dep': u'cc', u'dependent': 6, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'or'}, {u'dep': u'conj', u'dependent': 7, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'green'}, {u'dep': u'nmod', u'dependent': 8, u'governorGloss': u'fruit', u'governor': 1, u'dependentGloss': u'skin'}, {u'dep': u'cc', u'dependent': 9, u'governorGloss': u'fruit', u'governor': 1, u'dependentGloss': u'and'}, {u'dep': u'conj', u'dependent': 10, u'governorGloss': u'fruit', u'governor': 1, u'dependentGloss': u'sweet'}, {u'dep': u'case', u'dependent': 11, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'to'}, {u'dep': u'amod', u'dependent': 12, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'tart'}, {u'dep': u'amod', u'dependent': 13, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'crisp'}, {u'dep': u'amod', u'dependent': 14, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'whitish'}, {u'dep': u'nmod', u'dependent': 15, u'governorGloss': u'sweet', u'governor': 10, u'dependentGloss': u'flesh'}], u'parse': u'(ROOT\\r\\n  (UCP\\r\\n    (VP (VBN fruit)\\r\\n      (PP (IN with)\\r\\n        (NP\\r\\n          (ADJP (JJ red)\\r\\n            (CC or)\\r\\n            (JJ yellow)\\r\\n            (CC or)\\r\\n            (JJ green))\\r\\n          (NN skin))))\\r\\n    (CC and)\\r\\n    (ADJP (JJ sweet)\\r\\n      (PP (TO to)\\r\\n        (NP (JJ tart) (JJ crisp) (JJ whitish) (NN flesh))))))', u'collapsed-dependencies': [{u'dep': u'ROOT', u'dependent': 1, u'governorGloss': u'ROOT', u'governor': 0, u'dependentGloss': u'fruit'}, {u'dep': u'case', u'dependent': 2, u'governorGloss': u'skin', u'governor': 8, u'dependentGloss': u'with'}, {u'dep': u'dep', u'dependent': 3, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'red'}, {u'dep': u'cc', u'dependent': 4, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'or'}, {u'dep': u'amod', u'dependent': 5, u'governorGloss': u'skin', u'governor': 8, u'dependentGloss': u'yellow'}, {u'dep': u'cc', u'dependent': 6, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'or'}, {u'dep': u'conj:or', u'dependent': 7, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'green'}, {u'dep': u'nmod:with', u'dependent': 8, u'governorGloss': u'fruit', u'governor': 1, u'dependentGloss': u'skin'}, {u'dep': u'cc', u'dependent': 9, u'governorGloss': u'fruit', u'governor': 1, u'dependentGloss': u'and'}, {u'dep': u'conj:and', u'dependent': 10, u'governorGloss': u'fruit', u'governor': 1, u'dependentGloss': u'sweet'}, {u'dep': u'case', u'dependent': 11, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'to'}, {u'dep': u'amod', u'dependent': 12, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'tart'}, {u'dep': u'amod', u'dependent': 13, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'crisp'}, {u'dep': u'amod', u'dependent': 14, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'whitish'}, {u'dep': u'nmod:to', u'dependent': 15, u'governorGloss': u'sweet', u'governor': 10, u'dependentGloss': u'flesh'}], u'collapsed-ccprocessed-dependencies': [{u'dep': u'ROOT', u'dependent': 1, u'governorGloss': u'ROOT', u'governor': 0, u'dependentGloss': u'fruit'}, {u'dep': u'case', u'dependent': 2, u'governorGloss': u'skin', u'governor': 8, u'dependentGloss': u'with'}, {u'dep': u'dep', u'dependent': 3, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'red'}, {u'dep': u'cc', u'dependent': 4, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'or'}, {u'dep': u'amod', u'dependent': 5, u'governorGloss': u'skin', u'governor': 8, u'dependentGloss': u'yellow'}, {u'dep': u'cc', u'dependent': 6, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'or'}, {u'dep': u'conj:or', u'dependent': 7, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'green'}, {u'dep': u'amod', u'dependent': 7, u'governorGloss': u'skin', u'governor': 8, u'dependentGloss': u'green'}, {u'dep': u'nmod:with', u'dependent': 8, u'governorGloss': u'fruit', u'governor': 1, u'dependentGloss': u'skin'}, {u'dep': u'cc', u'dependent': 9, u'governorGloss': u'fruit', u'governor': 1, u'dependentGloss': u'and'}, {u'dep': u'conj:and', u'dependent': 10, u'governorGloss': u'fruit', u'governor': 1, u'dependentGloss': u'sweet'}, {u'dep': u'case', u'dependent': 11, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'to'}, {u'dep': u'amod', u'dependent': 12, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'tart'}, {u'dep': u'amod', u'dependent': 13, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'crisp'}, {u'dep': u'amod', u'dependent': 14, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'whitish'}, {u'dep': u'nmod:to', u'dependent': 15, u'governorGloss': u'sweet', u'governor': 10, u'dependentGloss': u'flesh'}]}]}\n"
     ]
    }
   ],
   "source": [
    "nlp=StanfordCoreNLP(nlpserver)\n",
    "properties={'annotators':'parse','outputFormat':'json'}\n",
    "\n",
    "#test CoreNLP server\n",
    "print nlp.annotate(\"fruit with red or yellow or green skin and sweet to tart crisp whitish flesh\",properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CoreNLP \"parse\" annotator JSON format\n",
    "...\n",
    "u'basic-dependencies':\n",
    "    {  u'dep': u'ROOT', \n",
    "    u'dependent': 4, #term ID\n",
    "    u'dependentGloss': u'pen', #term text\n",
    "    u'governor': 0, #\n",
    "    u'governorGloss': u'ROOT'},\n",
    "...\n",
    "\n",
    "\"\"\"\n",
    "class TermNode:\n",
    "    def __init__(self,param):\n",
    "        self.term=param[\"dependentGloss\"]\n",
    "        self.param=param\n",
    "        self.childs=[]\n",
    "    \n",
    "    def add_child(self,child):\n",
    "        self.childs.append(child)\n",
    "    \n",
    "    def find_id(self,node_id):\n",
    "        if self.param[\"dependent\"]==node_id:\n",
    "            return self\n",
    "        else:\n",
    "            for child in self.childs:\n",
    "                result=child.find_id(node_id)\n",
    "                if result!=None:\n",
    "                    return result\n",
    "            return None\n",
    "    \n",
    "    def get_training_data(self,weights,biases,keep_prob,word2vec_model):\n",
    "        #process child node\n",
    "        for child in self.childs:\n",
    "            child.get_training_data(weights,biases,keep_prob,word2vec_model)\n",
    "        \n",
    "        #calculate RNN output on this node\n",
    "        #calculate RNN output and use it for next input data\n",
    "        try:\n",
    "            rnn_result=word2vec_model[self.term]\n",
    "        except KeyError:\n",
    "            vector_size=len(word2vec_model[\"apple\"])\n",
    "            rnn_result=[0.0 for i in range(vector_size)]\n",
    "            \n",
    "        rnn_result=tf.convert_to_tensor([[value for value in rnn_result]])\n",
    "        no_droped=rnn_result\n",
    "        for child in self.childs:\n",
    "            #concatinate former iteration RNN result and next child node vector,and make it input data\n",
    "            x=tf.concat(1,[child.rnn_result,rnn_result])\n",
    "            rnn_result,no_droped=process_NN(weights,biases,keep_prob,x)\n",
    "            \n",
    "        #memorize final rnn output as feature for this node\n",
    "        self.rnn_result=rnn_result\n",
    "        self.no_droped=no_droped\n",
    "        \n",
    "\n",
    "def check_dependency_format(basic_dependency):\n",
    "    keys=[\"dependent\",\"governor\",\"dependentGloss\"]\n",
    "    for key in keys:\n",
    "        if not key in basic_dependency.keys():\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "#arg : basic-dependencies result of CoreNLP for a sentence\n",
    "#return : term tree structure  \n",
    "def get_term_tree(basic_dependencies):\n",
    "    #before processing checking result format\n",
    "    for basic_dependency in basic_dependencies:\n",
    "        if not check_dependency_format(basic_dependency):\n",
    "            return None\n",
    "        \n",
    "    root_node=TermNode(basic_dependencies[0])\n",
    "    node_dict={basic_dependencies[0][\"dependent\"]:root_node}\n",
    "    \n",
    "    #construct all node\n",
    "    for i in range(1,len(basic_dependencies)):\n",
    "        node_dict[basic_dependencies[i][\"dependent\"]]=TermNode(basic_dependencies[i])\n",
    "    \n",
    "    #make node into tree \n",
    "    for i in range(1,len(basic_dependencies)):\n",
    "        parent_node=node_dict[basic_dependencies[i][\"governor\"]]\n",
    "        parent_node.add_child(node_dict[basic_dependencies[i][\"dependent\"]])\n",
    " \n",
    "    return root_node\n",
    "\n",
    "# x: vector size*2\n",
    "#output : vector size\n",
    "def process_NN(weights,biases,keep_prob,x):\n",
    "    y_drop=x\n",
    "    for w,b in zip(weights,biases):\n",
    "        y=tf.nn.sigmoid(tf.matmul(y_drop,w)+b)\n",
    "        y_drop=tf.nn.dropout(y,keep_prob)\n",
    "    \n",
    "    return y_drop,y\n",
    "\n",
    "def printt(x):\n",
    "    print x.get_shape().dims,x.get_shape().ndims\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def build_auto_encoder(vector_size,lr,lambda_2):\n",
    "\n",
    "keep_prob=tf.placeholder(\"float\")\n",
    "\n",
    "def define_NN(input_size,layer_defs):\n",
    "    weights=[]\n",
    "    biases=[]\n",
    "    L2_sqr=0\n",
    "    prev_layer_size=input_size\n",
    "    for i,layer_size in enumerate(layer_defs):\n",
    "        with tf.name_scope(\"layer%d\"%i):\n",
    "            w_h=tf.Variable(tf.zeros([prev_layer_size,layer_size]))\n",
    "            b_h=tf.Variable(tf.zeros([layer_size]))\n",
    "        \n",
    "        L2_sqr=L2_sqr+tf.nn.l2_loss(w_h)\n",
    "        \n",
    "        weights.append(w_h)\n",
    "        biases.append(b_h)\n",
    "        prev_layer_size=layer_size\n",
    "    tf.scalar_summary(\"L2_loss\",L2_sqr)\n",
    "    \n",
    "    return weights,biases,L2_sqr\n",
    "\n",
    "def variable_summarizer(var,name):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.scalar_summary('mean/' + name, mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.scalar_summary('stddev/' + name, stddev)\n",
    "        tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "        tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "        tf.histogram_summary(name, var)\n",
    "    \n",
    "\n",
    "weights,biases,L2_sqr=define_NN(vector_size*2,[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load frequent term from term_file\n",
    "with open(term_file,\"r\") as f:\n",
    "    terms=[term.rstrip() for term in f.readlines() ]\n",
    "terms=terms[500:3500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [None, 363.65781, 363.65781]\n",
      "10 [None, 279.58701, 278.64001]\n",
      "20 [None, 417.34363, 416.11819]\n",
      "30 [None, 417.62372, 416.08737]\n",
      "40 [None, 306.57886, 304.90271]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e9df98a1a3e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mroot_node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess=tf.Session()\n",
    "sess.run(init)\n",
    "skipped=0\n",
    "\n",
    "for i in range(epoch):\n",
    "    term=terms[random.randrange(len(terms))]\n",
    "    try:\n",
    "        termVec=model[term]\n",
    "    except KeyError:\n",
    "        skipped+=1\n",
    "        continue\n",
    "    \n",
    "    synsets=wn.synsets(term)\n",
    "    if len(synsets)==0:\n",
    "        skipped+=1\n",
    "        continue\n",
    "    \n",
    "    #use first synset definition \n",
    "    definition=str(synsets[0].definition())\n",
    "    try:\n",
    "        annotated=nlp.annotate(definition,properties)\n",
    "\n",
    "    #error handling of core nlp\n",
    "    except UnicodeDecodeError:\n",
    "        skipped+=1\n",
    "        continue\n",
    "    if not isinstance(annotated,dict):\n",
    "        skipped+=1\n",
    "        continue\n",
    "    #use only first sentence\n",
    "    sentence=annotated[\"sentences\"][0]\n",
    "    root_node=get_term_tree(sentence[\"basic-dependencies\"])\n",
    "    if root_node==None:\n",
    "        skipped+=1\n",
    "        continue\n",
    "\n",
    "    #making feed back phase for RNN\n",
    "    root_node.get_training_data(weights,biases,keep_prob,model)\n",
    "\n",
    "    rnn_result=root_node.no_droped\n",
    "    true_label=model[term]\n",
    "    \n",
    "    cost=tf.reduce_mean(tf.reduce_sum(tf.pow(rnn_result-true_label,2)))\n",
    "    loss=cost+lambda_2*L2_sqr\n",
    "    train_step = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "   \n",
    "    #training\n",
    "    if i%10==0:\n",
    "        print i,sess.run([train_step,loss,cost],feed_dict={keep_prob:0.5})\n",
    "    else:\n",
    "        sess.run([train_step],feed_dict={keep_prob:0.5})\n",
    "    \n",
    "    del root_node\n",
    "    gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
