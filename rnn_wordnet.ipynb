{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from gensim.models import word2vec\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "corpus_file=\"data/enwiki-20150112-400-r10-105752.txt\"\n",
    "word2vec_model_file=\"word2vec_v2.mod\"\n",
    "vector_size=200\n",
    "nlpserver=\"http://localhost:9000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print wn.synsets(\"apple\")\n",
    "print wn.synsets(\"word\")[0].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#word2vec learning\n",
    "#load(or create) word2vec model \n",
    "def load_word2vec_model(model_file):\n",
    "    if os.path.exists(model_file):\n",
    "        model= word2vec.Word2Vec.load(model_file)\n",
    "    else:\n",
    "        data =word2vec.Text8Corpus(corpus_file)\n",
    "        model=word2vec.Word2Vec(data,size=vector_size)\n",
    "        model.save(model_file)\n",
    "    \n",
    "    return model\n",
    "\n",
    "#test\n",
    "model=load_word2vec_model(word2vec_model_file)\n",
    "out=model.most_similar(positive=[\"apple\"])\n",
    "for x in out:\n",
    "    print x[0],x[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp=StanfordCoreNLP(nlpserver)\n",
    "properties={'annotators':'parse','outputFormat':'json'}\n",
    "\n",
    "#test CoreNLP server\n",
    "print nlp.annotate(\"fruit with red or yellow or green skin and sweet to tart crisp whitish flesh\",properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CoreNLP \"parse\" annotator JSON format\n",
    "...\n",
    "u'basic-dependencies':\n",
    "    {  u'dep': u'ROOT', \n",
    "    u'dependent': 4, #term ID\n",
    "    u'dependentGloss': u'pen', #term text\n",
    "    u'governor': 0, #\n",
    "    u'governorGloss': u'ROOT'},\n",
    "...\n",
    "\n",
    "\"\"\"\n",
    "class TermNode:\n",
    "    def __init__(self,param):\n",
    "        self.term=param[\"dependentGloss\"]\n",
    "        self.param=param\n",
    "        self.childs=[]\n",
    "    \n",
    "    def add_child(self,child):\n",
    "        self.childs.append(child)\n",
    "    \n",
    "    def find_id(self,node_id):\n",
    "        if self.param[\"dependent\"]==node_id:\n",
    "            return self\n",
    "        else:\n",
    "            for child in self.childs:\n",
    "                result=child.find_id(node_id)\n",
    "                if result!=None:\n",
    "                    return result\n",
    "            return None\n",
    "    \n",
    "    def get_training_data(self,weights,biases,keep_prob,word2vec_model):\n",
    "        #process child node\n",
    "        for child in self.childs:\n",
    "            child.get_training_data(weights,biases,keep_prob,word2vec_model)\n",
    "        \n",
    "        #calculate RNN output on this node\n",
    "        #calculate RNN output and use it for next input data\n",
    "        try:\n",
    "            rnn_result=word2vec_model[self.term]\n",
    "        except KeyError:\n",
    "            vector_size=len(word2vec_model[\"apple\"])\n",
    "            rnn_result=[0 for i in range(vector_size)]\n",
    "        \n",
    "        rnn_result=tf.constant(rnn_result)\n",
    "        no_droped=rnn_result\n",
    "        for child in self.childs:\n",
    "            #concatinate former iteration RNN result and next child node vector,and make it training data\n",
    "            x=tf.concat(0,[child.rnn_result,rnn_result])\n",
    "            rnn_result,no_droped=process_NN(weights,biases,keep_prob,x)\n",
    "            \n",
    "        #memorize final rnn output as feature for this node\n",
    "        self.rnn_result=rnn_result\n",
    "        self.no_droped=no_droped\n",
    "        \n",
    "\n",
    "def check_dependency_format(basic_dependency):\n",
    "    keys=[\"dependent\",\"governor\",\"dependentGloss\"]\n",
    "    for key in keys:\n",
    "        if not key in basic_dependency.keys():\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "#arg : basic-dependencies result of CoreNLP for a sentence\n",
    "#return : term tree structure  \n",
    "def get_term_tree(basic_dependencies):\n",
    "    #before processing checking result format\n",
    "    for basic_dependency in basic_dependencies:\n",
    "        if not check_dependency_format(basic_dependency):\n",
    "            return None\n",
    "        \n",
    "    root_node=TermNode(basic_dependencies[0])\n",
    "    node_dict={basic_dependencies[0][\"dependent\"]:root_node}\n",
    "    \n",
    "    #construct all node\n",
    "    for i in range(1,len(basic_dependencies)):\n",
    "        node_dict[basic_dependencies[i][\"dependent\"]]=TermNode(basic_dependencies[i])\n",
    "    \n",
    "    #make node into tree \n",
    "    for i in range(1,len(basic_dependencies)):\n",
    "        parent_node=node_dict[basic_dependencies[i][\"governor\"]]\n",
    "        parent_node.add_child(node_dict[basic_dependencies[i][\"dependent\"]])\n",
    " \n",
    "    return root_node\n",
    "\n",
    "# x: vector size*2\n",
    "#output : vector size\n",
    "def process_NN(weights,biases,keep_prob,x):\n",
    "    y_drop=x\n",
    "    for w,b in zip(weights,biases):\n",
    "        y=tf.nn.sigmoid(tf.matmul(w,y_drop)+b)\n",
    "        y_drop=tf.nn.dropout(y,keep_prob)\n",
    "    \n",
    "    return y_drop,y\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def build_auto_encoder(vector_size,lr,lambda_2):\n",
    "\n",
    "keep_prob=tf.placeholder(\"float\")\n",
    "\n",
    "def define_NN(input_size,layer_defs):\n",
    "    weights=[]\n",
    "    biases=[]\n",
    "    L2_sqr=0\n",
    "    prev_layer_size=input_size\n",
    "    for layer_size in layer_defs:\n",
    "        w_h=tf.Variable(tf.zeros([prev_layer_size,layer_size]))\n",
    "        b_h=tf.Variable(tf.zeros([layer_size]))\n",
    "        L2_sqr=L2_sqr+tf.nn.l2_loss(w_h)\n",
    "        \n",
    "        \n",
    "        weights.append(w_h)\n",
    "        biases.append(b_h)\n",
    "        prev_layer_size=layer_size\n",
    "        \n",
    "    return weights,biases,L2_sqr\n",
    "\n",
    "weights,biases,L2_sqr=define_NN(vector_size*2,[200,200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess=tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "terms=[\"apple\"]\n",
    "\n",
    "for term in terms:\n",
    "    try:\n",
    "        termVec=model[\"term\"]\n",
    "    except KeyError:\n",
    "        continue\n",
    "    \n",
    "    synsets=wn.synsets(term)\n",
    "    if len(synsets)==0:\n",
    "        continue\n",
    "    \n",
    "    #use first synset definition \n",
    "    definition=str(synsets[0].definition())\n",
    "    print definition\n",
    "    try:\n",
    "        annotated=nlp.annotate(definition,properties)\n",
    "    \n",
    "    #error handling of core nlp\n",
    "    except UnicodeDecodeError:\n",
    "        continue\n",
    "    if not isinstance(annotated,dict):\n",
    "        continue\n",
    "    \n",
    "    #use only first sentence\n",
    "    sentence=annotated[\"sentences\"][0]\n",
    "    root_node=get_term_tree(sentence[\"basic-dependencies\"])\n",
    "    if root_node==None:\n",
    "        continue\n",
    "    \n",
    "    #making feed back phase for RNN\n",
    "    root_node.get_training_data(weights,biases,keep_prob,model)\n",
    "    rnn_result=root_node.non_droped\n",
    "    true_label=model[term]\n",
    "    cost=tf.reduce_mean(tf.reduce_sum(tf.pow(rnn_result-true_label,2)))\n",
    "    loss=cost+lambda_2*L2_sqr\n",
    "    train_step = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "    #training\n",
    "    session.run([train_step,],feed_dict={keep_prob:0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
