{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from gensim.models import word2vec\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "corpus_file=\"data/enwiki-20150112-400-r10-105752.txt\"\n",
    "word2vec_model_file=\"word2vec_v2.mod\"\n",
    "vector_size=200\n",
    "nlpserver=\"http://localhost:9000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('apple.n.01'), Synset('apple.n.02')]\n",
      "a unit of language that native speakers can identify\n"
     ]
    }
   ],
   "source": [
    "print wn.synsets(\"apple\")\n",
    "print wn.synsets(\"word\")[0].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple, 0.877130806446\n",
      "coconut 0.801503241062\n",
      "edible 0.800157427788\n",
      "almond 0.799016475677\n",
      "olive 0.797267079353\n",
      "bean 0.793999910355\n",
      "butter 0.790080070496\n",
      "onion 0.785118877888\n",
      "mango 0.783805608749\n",
      "fruit, 0.781246781349\n"
     ]
    }
   ],
   "source": [
    "#word2vec learning\n",
    "#load(or create) word2vec model \n",
    "def load_word2vec_model(model_file):\n",
    "    if os.path.exists(model_file):\n",
    "        model= word2vec.Word2Vec.load(model_file)\n",
    "    else:\n",
    "        data =word2vec.Text8Corpus(corpus_file)\n",
    "        model=word2vec.Word2Vec(data,size=vector_size)\n",
    "        model.save(model_file)\n",
    "    \n",
    "    return model\n",
    "\n",
    "#test\n",
    "model=load_word2vec_model(word2vec_model_file)\n",
    "out=model.most_similar(positive=[\"apple\"])\n",
    "for x in out:\n",
    "    print x[0],x[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'sentences': [{u'tokens': [{u'index': 1, u'word': u'fruit', u'after': u' ', u'pos': u'VBN', u'characterOffsetEnd': 5, u'characterOffsetBegin': 0, u'originalText': u'fruit', u'before': u''}, {u'index': 2, u'word': u'with', u'after': u' ', u'pos': u'IN', u'characterOffsetEnd': 10, u'characterOffsetBegin': 6, u'originalText': u'with', u'before': u' '}, {u'index': 3, u'word': u'red', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 14, u'characterOffsetBegin': 11, u'originalText': u'red', u'before': u' '}, {u'index': 4, u'word': u'or', u'after': u' ', u'pos': u'CC', u'characterOffsetEnd': 17, u'characterOffsetBegin': 15, u'originalText': u'or', u'before': u' '}, {u'index': 5, u'word': u'yellow', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 24, u'characterOffsetBegin': 18, u'originalText': u'yellow', u'before': u' '}, {u'index': 6, u'word': u'or', u'after': u' ', u'pos': u'CC', u'characterOffsetEnd': 27, u'characterOffsetBegin': 25, u'originalText': u'or', u'before': u' '}, {u'index': 7, u'word': u'green', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 33, u'characterOffsetBegin': 28, u'originalText': u'green', u'before': u' '}, {u'index': 8, u'word': u'skin', u'after': u' ', u'pos': u'NN', u'characterOffsetEnd': 38, u'characterOffsetBegin': 34, u'originalText': u'skin', u'before': u' '}, {u'index': 9, u'word': u'and', u'after': u' ', u'pos': u'CC', u'characterOffsetEnd': 42, u'characterOffsetBegin': 39, u'originalText': u'and', u'before': u' '}, {u'index': 10, u'word': u'sweet', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 48, u'characterOffsetBegin': 43, u'originalText': u'sweet', u'before': u' '}, {u'index': 11, u'word': u'to', u'after': u' ', u'pos': u'TO', u'characterOffsetEnd': 51, u'characterOffsetBegin': 49, u'originalText': u'to', u'before': u' '}, {u'index': 12, u'word': u'tart', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 56, u'characterOffsetBegin': 52, u'originalText': u'tart', u'before': u' '}, {u'index': 13, u'word': u'crisp', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 62, u'characterOffsetBegin': 57, u'originalText': u'crisp', u'before': u' '}, {u'index': 14, u'word': u'whitish', u'after': u' ', u'pos': u'JJ', u'characterOffsetEnd': 70, u'characterOffsetBegin': 63, u'originalText': u'whitish', u'before': u' '}, {u'index': 15, u'word': u'flesh', u'after': u'', u'pos': u'NN', u'characterOffsetEnd': 76, u'characterOffsetBegin': 71, u'originalText': u'flesh', u'before': u' '}], u'index': 0, u'basic-dependencies': [{u'dep': u'ROOT', u'dependent': 1, u'governorGloss': u'ROOT', u'governor': 0, u'dependentGloss': u'fruit'}, {u'dep': u'case', u'dependent': 2, u'governorGloss': u'skin', u'governor': 8, u'dependentGloss': u'with'}, {u'dep': u'dep', u'dependent': 3, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'red'}, {u'dep': u'cc', u'dependent': 4, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'or'}, {u'dep': u'amod', u'dependent': 5, u'governorGloss': u'skin', u'governor': 8, u'dependentGloss': u'yellow'}, {u'dep': u'cc', u'dependent': 6, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'or'}, {u'dep': u'conj', u'dependent': 7, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'green'}, {u'dep': u'nmod', u'dependent': 8, u'governorGloss': u'fruit', u'governor': 1, u'dependentGloss': u'skin'}, {u'dep': u'cc', u'dependent': 9, u'governorGloss': u'fruit', u'governor': 1, u'dependentGloss': u'and'}, {u'dep': u'conj', u'dependent': 10, u'governorGloss': u'fruit', u'governor': 1, u'dependentGloss': u'sweet'}, {u'dep': u'case', u'dependent': 11, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'to'}, {u'dep': u'amod', u'dependent': 12, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'tart'}, {u'dep': u'amod', u'dependent': 13, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'crisp'}, {u'dep': u'amod', u'dependent': 14, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'whitish'}, {u'dep': u'nmod', u'dependent': 15, u'governorGloss': u'sweet', u'governor': 10, u'dependentGloss': u'flesh'}], u'parse': u'(ROOT\\r\\n  (UCP\\r\\n    (VP (VBN fruit)\\r\\n      (PP (IN with)\\r\\n        (NP\\r\\n          (ADJP (JJ red)\\r\\n            (CC or)\\r\\n            (JJ yellow)\\r\\n            (CC or)\\r\\n            (JJ green))\\r\\n          (NN skin))))\\r\\n    (CC and)\\r\\n    (ADJP (JJ sweet)\\r\\n      (PP (TO to)\\r\\n        (NP (JJ tart) (JJ crisp) (JJ whitish) (NN flesh))))))', u'collapsed-dependencies': [{u'dep': u'ROOT', u'dependent': 1, u'governorGloss': u'ROOT', u'governor': 0, u'dependentGloss': u'fruit'}, {u'dep': u'case', u'dependent': 2, u'governorGloss': u'skin', u'governor': 8, u'dependentGloss': u'with'}, {u'dep': u'dep', u'dependent': 3, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'red'}, {u'dep': u'cc', u'dependent': 4, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'or'}, {u'dep': u'amod', u'dependent': 5, u'governorGloss': u'skin', u'governor': 8, u'dependentGloss': u'yellow'}, {u'dep': u'cc', u'dependent': 6, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'or'}, {u'dep': u'conj:or', u'dependent': 7, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'green'}, {u'dep': u'nmod:with', u'dependent': 8, u'governorGloss': u'fruit', u'governor': 1, u'dependentGloss': u'skin'}, {u'dep': u'cc', u'dependent': 9, u'governorGloss': u'fruit', u'governor': 1, u'dependentGloss': u'and'}, {u'dep': u'conj:and', u'dependent': 10, u'governorGloss': u'fruit', u'governor': 1, u'dependentGloss': u'sweet'}, {u'dep': u'case', u'dependent': 11, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'to'}, {u'dep': u'amod', u'dependent': 12, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'tart'}, {u'dep': u'amod', u'dependent': 13, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'crisp'}, {u'dep': u'amod', u'dependent': 14, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'whitish'}, {u'dep': u'nmod:to', u'dependent': 15, u'governorGloss': u'sweet', u'governor': 10, u'dependentGloss': u'flesh'}], u'collapsed-ccprocessed-dependencies': [{u'dep': u'ROOT', u'dependent': 1, u'governorGloss': u'ROOT', u'governor': 0, u'dependentGloss': u'fruit'}, {u'dep': u'case', u'dependent': 2, u'governorGloss': u'skin', u'governor': 8, u'dependentGloss': u'with'}, {u'dep': u'dep', u'dependent': 3, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'red'}, {u'dep': u'cc', u'dependent': 4, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'or'}, {u'dep': u'amod', u'dependent': 5, u'governorGloss': u'skin', u'governor': 8, u'dependentGloss': u'yellow'}, {u'dep': u'cc', u'dependent': 6, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'or'}, {u'dep': u'conj:or', u'dependent': 7, u'governorGloss': u'yellow', u'governor': 5, u'dependentGloss': u'green'}, {u'dep': u'amod', u'dependent': 7, u'governorGloss': u'skin', u'governor': 8, u'dependentGloss': u'green'}, {u'dep': u'nmod:with', u'dependent': 8, u'governorGloss': u'fruit', u'governor': 1, u'dependentGloss': u'skin'}, {u'dep': u'cc', u'dependent': 9, u'governorGloss': u'fruit', u'governor': 1, u'dependentGloss': u'and'}, {u'dep': u'conj:and', u'dependent': 10, u'governorGloss': u'fruit', u'governor': 1, u'dependentGloss': u'sweet'}, {u'dep': u'case', u'dependent': 11, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'to'}, {u'dep': u'amod', u'dependent': 12, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'tart'}, {u'dep': u'amod', u'dependent': 13, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'crisp'}, {u'dep': u'amod', u'dependent': 14, u'governorGloss': u'flesh', u'governor': 15, u'dependentGloss': u'whitish'}, {u'dep': u'nmod:to', u'dependent': 15, u'governorGloss': u'sweet', u'governor': 10, u'dependentGloss': u'flesh'}]}]}\n"
     ]
    }
   ],
   "source": [
    "nlp=StanfordCoreNLP(nlpserver)\n",
    "properties={'annotators':'parse','outputFormat':'json'}\n",
    "\n",
    "#test CoreNLP server\n",
    "print nlp.annotate(\"fruit with red or yellow or green skin and sweet to tart crisp whitish flesh\",properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CoreNLP \"parse\" annotator JSON format\n",
    "...\n",
    "u'basic-dependencies':\n",
    "    {  u'dep': u'ROOT', \n",
    "    u'dependent': 4, #term ID\n",
    "    u'dependentGloss': u'pen', #term text\n",
    "    u'governor': 0, #\n",
    "    u'governorGloss': u'ROOT'},\n",
    "...\n",
    "\n",
    "\"\"\"\n",
    "class TermNode:\n",
    "    def __init__(self,param):\n",
    "        self.term=param[\"dependentGloss\"]\n",
    "        self.param=param\n",
    "        self.childs=[]\n",
    "    \n",
    "    def add_child(self,child):\n",
    "        self.childs.append(child)\n",
    "    \n",
    "    def find_id(self,node_id):\n",
    "        if self.param[\"dependent\"]==node_id:\n",
    "            return self\n",
    "        else:\n",
    "            for child in self.childs:\n",
    "                result=child.find_id(node_id)\n",
    "                if result!=None:\n",
    "                    return result\n",
    "            return None\n",
    "    \n",
    "    def get_training_data(self,weights,biases,keep_prob,word2vec_model):\n",
    "        #process child node\n",
    "        for child in self.childs:\n",
    "            child.get_training_data(weights,biases,keep_prob,word2vec_model)\n",
    "        \n",
    "        #calculate RNN output on this node\n",
    "        #calculate RNN output and use it for next input data\n",
    "        try:\n",
    "            rnn_result=word2vec_model[self.term]\n",
    "        except KeyError:\n",
    "            vector_size=len(word2vec_model[\"apple\"])\n",
    "            rnn_result=[0 for i in range(vector_size)]\n",
    "        \n",
    "        rnn_result=tf.constant(rnn_result)\n",
    "        no_droped=rnn_result\n",
    "        for child in self.childs:\n",
    "            #concatinate former iteration RNN result and next child node vector,and make it training data\n",
    "            x=tf.concat(0,[child.rnn_result,rnn_result])\n",
    "            rnn_result,no_droped=process_NN(weights,biases,keep_prob,x)\n",
    "            \n",
    "        #memorize final rnn output as feature for this node\n",
    "        self.rnn_result=rnn_result\n",
    "        self.no_droped=no_droped\n",
    "        \n",
    "\n",
    "def check_dependency_format(basic_dependency):\n",
    "    keys=[\"dependent\",\"governor\",\"dependentGloss\"]\n",
    "    for key in keys:\n",
    "        if not key in basic_dependency.keys():\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "#arg : basic-dependencies result of CoreNLP for a sentence\n",
    "#return : term tree structure  \n",
    "def get_term_tree(basic_dependencies):\n",
    "    #before processing checking result format\n",
    "    for basic_dependency in basic_dependencies:\n",
    "        if not check_dependency_format(basic_dependency):\n",
    "            return None\n",
    "        \n",
    "    root_node=TermNode(basic_dependencies[0])\n",
    "    node_dict={basic_dependencies[0][\"dependent\"]:root_node}\n",
    "    \n",
    "    #construct all node\n",
    "    for i in range(1,len(basic_dependencies)):\n",
    "        node_dict[basic_dependencies[i][\"dependent\"]]=TermNode(basic_dependencies[i])\n",
    "    \n",
    "    #make node into tree \n",
    "    for i in range(1,len(basic_dependencies)):\n",
    "        parent_node=node_dict[basic_dependencies[i][\"governor\"]]\n",
    "        parent_node.add_child(node_dict[basic_dependencies[i][\"dependent\"]])\n",
    " \n",
    "    return root_node\n",
    "\n",
    "# x: vector size*2\n",
    "#output : vector size\n",
    "def process_NN(weights,biases,keep_prob,x):\n",
    "    y_drop=x\n",
    "    for w,b in zip(weights,biases):\n",
    "        y=tf.nn.sigmoid(tf.matmul(w,y_drop)+b)\n",
    "        y_drop=tf.nn.dropout(y,keep_prob)\n",
    "    \n",
    "    return y_drop,y\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def build_auto_encoder(vector_size,lr,lambda_2):\n",
    "\n",
    "keep_prob=tf.placeholder(\"float\")\n",
    "\n",
    "def define_NN(input_size,layer_defs):\n",
    "    weights=[]\n",
    "    biases=[]\n",
    "    L2_sqr=0\n",
    "    prev_layer_size=input_size\n",
    "    for layer_size in layer_defs:\n",
    "        w_h=tf.Variable(tf.zeros([prev_layer_size,layer_size]))\n",
    "        b_h=tf.Variable(tf.zeros([layer_size]))\n",
    "        L2_sqr=L2_sqr+tf.nn.l2_loss(w_h)\n",
    "        \n",
    "        \n",
    "        weights.append(w_h)\n",
    "        biases.append(b_h)\n",
    "        prev_layer_size=layer_size\n",
    "        \n",
    "    return weights,biases,L2_sqr\n",
    "\n",
    "weights,biases,L2_sqr=define_NN(vector_size*2,[200,200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fruit with red or yellow or green skin and sweet to tart crisp whitish flesh\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape (400,) must have rank 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-0f1dc3b30f9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m#making feed back phase for RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mroot_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mrnn_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_droped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mtrue_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-dc46d8d7489d>\u001b[0m in \u001b[0;36mget_training_data\u001b[0;34m(self, weights, biases, keep_prob, word2vec_model)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m#process child node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchilds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword2vec_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m#calculate RNN output on this node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-dc46d8d7489d>\u001b[0m in \u001b[0;36mget_training_data\u001b[0;34m(self, weights, biases, keep_prob, word2vec_model)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m#process child node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchilds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword2vec_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m#calculate RNN output on this node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-dc46d8d7489d>\u001b[0m in \u001b[0;36mget_training_data\u001b[0;34m(self, weights, biases, keep_prob, word2vec_model)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;31m#concatinate former iteration RNN result and next child node vector,and make it training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_result\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnn_result\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mrnn_result\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mno_droped\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess_NN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m#memorize final rnn output as feature for this node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-dc46d8d7489d>\u001b[0m in \u001b[0;36mprocess_NN\u001b[0;34m(weights, biases, keep_prob, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0my_drop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0my_drop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/takashi/.pyenv/versions/2.7.12/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.pyc\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   1350\u001b[0m                                    \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m                                    \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m                                    name=name)\n\u001b[0m\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m \u001b[0msparse_matmul\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_mat_mul\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/takashi/.pyenv/versions/2.7.12/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.pyc\u001b[0m in \u001b[0;36m_mat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   1294\u001b[0m   \"\"\"\n\u001b[1;32m   1295\u001b[0m   result = _op_def_lib.apply_op(\"MatMul\", a=a, b=b, transpose_a=transpose_a,\n\u001b[0;32m-> 1296\u001b[0;31m                                 transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   1297\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/takashi/.pyenv/versions/2.7.12/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    701\u001b[0m           op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    702\u001b[0m                            \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m                            op_def=op_def)\n\u001b[0m\u001b[1;32m    704\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m           return _Restructure(ops.convert_n_to_tensor(outputs),\n",
      "\u001b[0;32m/home/takashi/.pyenv/versions/2.7.12/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2317\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2319\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2320\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2321\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/takashi/.pyenv/versions/2.7.12/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1709\u001b[0m       raise RuntimeError(\"No shape function registered for standard op: %s\"\n\u001b[1;32m   1710\u001b[0m                          % op.type)\n\u001b[0;32m-> 1711\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1712\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1713\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/home/takashi/.pyenv/versions/2.7.12/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36mmatmul_shape\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0ma_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m   \u001b[0mtranspose_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transpose_a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m   \u001b[0mb_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m   \u001b[0mtranspose_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transpose_b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   \u001b[0moutput_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtranspose_a\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0ma_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/takashi/.pyenv/versions/2.7.12/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.pyc\u001b[0m in \u001b[0;36mwith_rank\u001b[0;34m(self, rank)\u001b[0m\n\u001b[1;32m    639\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munknown_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape %s must have rank %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwith_rank_at_least\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape (400,) must have rank 2"
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess=tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "terms=[\"apple\"]\n",
    "\n",
    "for term in terms:\n",
    "    try:\n",
    "        termVec=model[\"term\"]\n",
    "    except KeyError:\n",
    "        continue\n",
    "    \n",
    "    synsets=wn.synsets(term)\n",
    "    if len(synsets)==0:\n",
    "        continue\n",
    "    \n",
    "    #use first synset definition \n",
    "    definition=str(synsets[0].definition())\n",
    "    print definition\n",
    "    try:\n",
    "        annotated=nlp.annotate(definition,properties)\n",
    "    \n",
    "    #error handling of core nlp\n",
    "    except UnicodeDecodeError:\n",
    "        continue\n",
    "    if not isinstance(annotated,dict):\n",
    "        continue\n",
    "    \n",
    "    #use only first sentence\n",
    "    sentence=annotated[\"sentences\"][0]\n",
    "    root_node=get_term_tree(sentence[\"basic-dependencies\"])\n",
    "    if root_node==None:\n",
    "        continue\n",
    "    \n",
    "    #making feed back phase for RNN\n",
    "    root_node.get_training_data(weights,biases,keep_prob,model)\n",
    "    rnn_result=root_node.non_droped\n",
    "    true_label=model[term]\n",
    "    cost=tf.reduce_mean(tf.reduce_sum(tf.pow(rnn_result-true_label,2)))\n",
    "    loss=cost+lambda_2*L2_sqr\n",
    "    train_step = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "    #training\n",
    "    session.run([train_step,],feed_dict={keep_prob:0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
